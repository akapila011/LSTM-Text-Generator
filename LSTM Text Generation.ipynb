{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the text and clean the data (capitilization, unnecessary characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text has 128743 characters\n"
     ]
    }
   ],
   "source": [
    "filepath = \"dataset/wows-script.txt\"\n",
    "with open(filepath, \"r\") as f:\n",
    "    raw_text = f.read().lower()\n",
    "print(\"Text has {} characters\".format(len(raw_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must convert characters to integers so that the Neural Network can work with the data.\n",
    "\n",
    "Create a vocabulary, mapping for each unique character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique mappings(characters): 52\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(list(set(raw_text))) # sorted list of unique chars\n",
    "char_to_int = dict((c, i) for i, c in enumerate(characters)) # e.g. a: 1, b:2\n",
    "print(\"Number of unique mappings(characters): {}\".format(len(char_to_int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break up text in to sequences of 100 characters.\n",
    "\n",
    "There are also other options like splitting up by sentences and padding shorter sentences/truncating longer sentences.\n",
    "\n",
    "When training, it will use 100 time steps for a single character to give a single output. We move along the text 1 character at a time. Each character will be learned from the preceding 100 characters (except the first 100 characters)\n",
    "\n",
    "E.g. seq_length = 4\n",
    "Hell -> Hello\n",
    "\n",
    "Now convert the data set from characters to integer representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns 128643\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "data_x = []\n",
    "data_y = []\n",
    "for i in range(0, (len(raw_text)-seq_length), 1):\n",
    "    seq_in = raw_text[i:i+seq_length] # x\n",
    "    seq_out = raw_text[i+seq_length]  # y\n",
    "    data_x.append([char_to_int[char] for char in seq_in])\n",
    "    data_y.append(char_to_int[seq_out])\n",
    "n_patterns = len(data_x)\n",
    "print(\"Total patterns {}\".format(n_patterns))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
    "\n",
    "Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
    "\n",
    "Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the 47 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(data_x, (n_patterns, seq_length, 1)) # reshape to  [samples, time steps, features]\n",
    "X = X / float(len(char_to_int))  # normalize values\n",
    "y = np_utils.to_categorical(data_y)  # one hot encode the output variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network architecture\n",
    "\n",
    "- Single hidden layer with 256 units\n",
    "- Droput with 20% probablity\n",
    "- Output layer is Dense using softmax activation to output a probablity predicition for each of the 47 characters between 0 & 1\n",
    "\n",
    "*The problem is really a single character classification problem with 47 classes and as such is defined as optimizing the log loss (cross entropy), here using the ADAM optimization algorithm for speed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 52)                13364     \n",
      "=================================================================\n",
      "Total params: 277,556\n",
      "Trainable params: 277,556\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not interested in most accurate model (classification accuracy). Looking for a model that generlizes the dataset - that minimizes the loss. Seeking a balance between generlaization and overfitting but short of memorization.\n",
    "\n",
    "Training can be slow so we use a model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = \"checkpoints/weights-imporvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_filepath, monitor=\"loss\", verbose=1, save_best_only=True)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epocks=20, batch_size=128, callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
